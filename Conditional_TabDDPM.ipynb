{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conditional Generation of time_to_event using TabDDPM Architecture\n",
    "\n",
    "In this notebook, we demonstrate how to build a conditional diffusion model that generates the `time_to_event` for a patient conditioned on both categorical and numerical features. This notebook follows the TabDDPM spirit by combining these feature types while inverting the typical supervised regression problem.\n",
    "\n",
    "The steps include:\n",
    "\n",
    "1. **Data Preprocessing:** Loading the unos liver transplant dataset, separating out the target (`time_to_event`), scaling numerical features, and one-hot encoding categorical features. All non-target features form the conditioning vector.\n",
    "2. **Diffusion Setup:** Defining a cosine beta schedule and helper functions to add noise to the target.\n",
    "3. **Model Definition:** Building a denoising network (an MLP) that takes as input the noisy target, a sinusoidal time embedding, and the fixed condition vector, and predicts the noise that was added.\n",
    "4. **Training:** Using a DDPM-style training loop with an MSE loss between the predicted noise and the true noise.\n",
    "5. **Sampling:** Implementing a reverse diffusion (using a simple DDIM sampler) that generates a plausible `time_to_event` given a condition.\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Use GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Preprocess Dataset\n",
    "\n",
    "We assume a CSV file `unos_liver_transplant.csv` is available with these columns:\n",
    "\n",
    "- **Numerical:** INIT_AGE, INIT_BMI_CALC, INIT_SERUM_SODIUM, INIT_SERUM_CREAT, INIT_INR, INIT_BILIRUBIN, INIT_ALBUMIN, INIT_MELD, time_to_event\n",
    "- **Categorical:** GENDER, ABO, Etiology, Ethnicity, diab_group_labeled, Encephalopathy_Status, Ascites_Status, INIT_DIALYSIS_PRIOR_WEEK\n",
    "\n",
    "We separate `time_to_event` as the target and combine the remaining numerical features (after scaling) with one-hot encoded categorical features to form our condition vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load dataset (update the path as needed)\n",
    "data_path = 'unos_liver_transplant.csv'\n",
    "if os.path.exists(data_path):\n",
    "    df = pd.read_csv(data_path)\n",
    "else:\n",
    "    # For demonstration, create a dummy dataframe with similar structure\n",
    "    num_data = {\n",
    "        'INIT_AGE': np.random.randint(20, 80, 1000),\n",
    "        'INIT_BMI_CALC': np.random.uniform(18, 35, 1000),\n",
    "        'INIT_SERUM_SODIUM': np.random.uniform(120, 145, 1000),\n",
    "        'INIT_SERUM_CREAT': np.random.uniform(0.5, 2.0, 1000),\n",
    "        'INIT_INR': np.random.uniform(0.8, 2.5, 1000),\n",
    "        'INIT_BILIRUBIN': np.random.uniform(0.5, 5.0, 1000),\n",
    "        'INIT_ALBUMIN': np.random.uniform(2.5, 4.5, 1000),\n",
    "        'INIT_MELD': np.random.uniform(6, 40, 1000),\n",
    "        'time_to_event': np.random.uniform(0, 3650, 1000)\n",
    "    }\n",
    "    cat_data = {\n",
    "        'GENDER': np.random.choice(['M', 'F'], 1000),\n",
    "        'ABO': np.random.choice(['A', 'B', 'O'], 1000),\n",
    "        'Etiology': np.random.choice(['Alcohol', 'Autoimmune', 'Cryptogenic', 'NASH'], 1000),\n",
    "        'Ethnicity': np.random.choice(['White', 'Black/African American', 'Hispanic/Latino'], 1000),\n",
    "        'diab_group_labeled': np.random.choice(['Diabetes', 'No Diabetes'], 1000),\n",
    "        'Encephalopathy_Status': np.random.choice(['Grade 1-2', 'Grade 3-4', 'NaN'], 1000),\n",
    "        'Ascites_Status': np.random.choice(['Slight', 'Moderate', 'NaN'], 1000),\n",
    "        'INIT_DIALYSIS_PRIOR_WEEK': np.random.choice([0, 1], 1000)\n",
    "    }\n",
    "    df = pd.DataFrame({**num_data, **cat_data})\n",
    "    print('Dummy dataset created')\n",
    "\n",
    "# Specify columns\n",
    "target_col = 'time_to_event'\n",
    "numerical_cols = [\n",
    "    'INIT_AGE', 'INIT_BMI_CALC', 'INIT_SERUM_SODIUM',\n",
    "    'INIT_SERUM_CREAT', 'INIT_INR', 'INIT_BILIRUBIN',\n",
    "    'INIT_ALBUMIN', 'INIT_MELD'\n",
    "]\n",
    "categorical_cols = [\n",
    "    'GENDER', 'ABO', 'Etiology', 'Ethnicity',\n",
    "    'diab_group_labeled', 'Encephalopathy_Status',\n",
    "    'Ascites_Status', 'INIT_DIALYSIS_PRIOR_WEEK'\n",
    "]\n",
    "\n",
    "# Extract target (time_to_event)\n",
    "y = df[target_col].values.astype(np.float32).reshape(-1, 1)\n",
    "\n",
    "# Process numerical features: scale them (simple min-max scaling)\n",
    "num_features = df[numerical_cols].values.astype(np.float32)\n",
    "num_min = num_features.min(axis=0)\n",
    "num_max = num_features.max(axis=0)\n",
    "num_scaled = (num_features - num_min) / (num_max - num_min + 1e-8)\n",
    "\n",
    "# Process categorical features using one-hot encoding\n",
    "cat_df = pd.get_dummies(df[categorical_cols], drop_first=True)\n",
    "cat_features = cat_df.values.astype(np.float32)\n",
    "\n",
    "# Combine conditioning features: numerical (excluding target) + categorical\n",
    "condition = np.hstack([num_scaled, cat_features])\n",
    "print('Condition shape:', condition.shape)\n",
    "print('Target shape:', y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create PyTorch Dataset and DataLoader\n",
    "\n",
    "We create a simple Dataset that returns a tuple of `(target, condition)` for each patient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class TransplantDataset(Dataset):\n",
    "    def __init__(self, condition, target):\n",
    "        self.condition = torch.tensor(condition, dtype=torch.float32)\n",
    "        self.target = torch.tensor(target, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.target)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.target[idx], self.condition[idx]\n",
    "\n",
    "dataset = TransplantDataset(condition, y)\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "print(f'Dataset size: {len(dataset)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Diffusion Hyperparameters and Helper Functions\n",
    "\n",
    "We use a cosine schedule to compute the beta values and then derive the corresponding alphas and cumulative products. The function `q_sample` adds noise to the target value based on a given timestep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "T = 1000  # total diffusion timesteps\n",
    "\n",
    "def get_beta_schedule_cosine(T):\n",
    "    steps = T\n",
    "    s = 0.008\n",
    "    t = np.linspace(0, steps, steps+1) / steps\n",
    "    alphas_cumprod = np.cos((t + s) / (1 + s) * np.pi / 2) ** 2\n",
    "    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
    "    betas = []\n",
    "    for i in range(steps):\n",
    "        beta = 1 - alphas_cumprod[i+1] / alphas_cumprod[i]\n",
    "        betas.append(min(beta, 0.999))\n",
    "    return np.array(betas, dtype=np.float32)\n",
    "\n",
    "betas = get_beta_schedule_cosine(T)\n",
    "alphas = 1 - betas\n",
    "alphas_cumprod = np.cumprod(alphas)\n",
    "\n",
    "def q_sample(x0, t, noise):\n",
    "    \"\"\"\n",
    "    Diffuse the data at timestep t\n",
    "    x0: original data (batch, 1)\n",
    "    t: timestep tensor (batch,)\n",
    "    noise: noise tensor\n",
    "    \"\"\"\n",
    "    # Convert cumulative products to tensors\n",
    "    sqrt_alphas_cumprod = torch.tensor(np.sqrt(alphas_cumprod), dtype=torch.float32, device=x0.device)\n",
    "    sqrt_one_minus_alphas_cumprod = torch.tensor(np.sqrt(1 - alphas_cumprod), dtype=torch.float32, device=x0.device)\n",
    "    a = sqrt_alphas_cumprod[t].unsqueeze(1)\n",
    "    b = sqrt_one_minus_alphas_cumprod[t].unsqueeze(1)\n",
    "    return a * x0 + b * noise\n",
    "\n",
    "def sample_t(batch_size):\n",
    "    return torch.randint(0, T, (batch_size,), device=device).long()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Time Embedding\n",
    "\n",
    "We define a sinusoidal time embedding function to inject information about the diffusion timestep into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def get_timestep_embedding(timesteps, embedding_dim):\n",
    "    \"\"\"\n",
    "    Returns a sinusoidal embedding for t\n",
    "    \"\"\"\n",
    "    half_dim = embedding_dim // 2\n",
    "    emb = np.log(10000) / (half_dim - 1)\n",
    "    emb = torch.exp(torch.arange(half_dim, device=timesteps.device, dtype=torch.float32) * -emb)\n",
    "    emb = timesteps.float().unsqueeze(1) * emb.unsqueeze(0)\n",
    "    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)\n",
    "    if embedding_dim % 2 == 1:\n",
    "        emb = torch.cat([emb, torch.zeros(timesteps.size(0), 1, device=timesteps.device)], dim=1)\n",
    "    return emb\n",
    "\n",
    "embedding_dim = 32  # dimension of time embedding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Define the Denoising Network (MLP)\n",
    "\n",
    "The network takes as input the concatenation of:\n",
    "- A noisy version of the target (shape: [batch, 1])\n",
    "- The condition vector (combined numerical and categorical features)\n",
    "- A time embedding for the current diffusion timestep\n",
    "\n",
    "It outputs a prediction of the noise that was added to the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class DenoiseMLP(nn.Module):\n",
    "    def __init__(self, condition_dim, time_emb_dim, hidden_dim=128):\n",
    "        super(DenoiseMLP, self).__init__()\n",
    "        input_dim = 1 + condition_dim + time_emb_dim  # 1 for target\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)  # output: predicted noise\n",
    "        )\n",
    "\n",
    "    def forward(self, x_target_noisy, condition, t):\n",
    "        # Get time embedding\n",
    "        t_emb = get_timestep_embedding(t, embedding_dim)\n",
    "        # Concatenate along feature dimension\n",
    "        x = torch.cat([x_target_noisy, condition, t_emb], dim=1)\n",
    "        return self.net(x)\n",
    "\n",
    "# Determine condition dimension\n",
    "condition_dim = condition.shape[1]\n",
    "model = DenoiseMLP(condition_dim, embedding_dim).to(device)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Loop\n",
    "\n",
    "We now train the model to predict the noise added to the target value. For each batch:\n",
    "\n",
    "1. Sample a random diffusion timestep `t` for every example.\n",
    "2. Sample Gaussian noise and add it to the true target using the `q_sample` function.\n",
    "3. Feed the noisy target, condition vector, and timestep into the network to predict the noise.\n",
    "4. Compute the MSE loss between the predicted and true noise and backpropagate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "num_epochs = 5  # for demonstration purposes\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    for x_target, cond in dataloader:\n",
    "        x_target = x_target.to(device)  # shape: (batch, 1)\n",
    "        cond = cond.to(device)          # shape: (batch, condition_dim)\n",
    "        batch_size = x_target.size(0)\n",
    "        t = sample_t(batch_size)        # random timesteps for each sample\n",
    "        noise = torch.randn_like(x_target)\n",
    "        x_target_noisy = q_sample(x_target, t, noise)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        noise_pred = model(x_target_noisy, cond, t)\n",
    "        loss = nn.MSELoss()(noise_pred, noise)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item() * batch_size\n",
    "    \n",
    "    epoch_loss /= len(dataset)\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Sampling: Generate time_to_event for a Given Condition\n",
    "\n",
    "Next, we implement a reverse diffusion (sampling) procedure. Starting from random noise, we iterate through the timesteps (using a DDIM-style update with eta=0 for determinism) to generate a sample for `time_to_event` given a condition vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def p_sample(model, x, cond, t):\n",
    "    \"\"\"\n",
    "    One reverse diffusion step.\n",
    "    x: current noisy sample (batch, 1)\n",
    "    cond: condition vector (batch, condition_dim)\n",
    "    t: current timestep tensor (batch,)\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        noise_pred = model(x, cond, t)\n",
    "        # Compute predicted x0\n",
    "        sqrt_alphas_cumprod_t = torch.tensor(np.sqrt(alphas_cumprod), dtype=torch.float32, device=x.device)[t].unsqueeze(1)\n",
    "        sqrt_one_minus_alphas_cumprod_t = torch.tensor(np.sqrt(1 - alphas_cumprod), dtype=torch.float32, device=x.device)[t].unsqueeze(1)\n",
    "        x0_pred = (x - sqrt_one_minus_alphas_cumprod_t * noise_pred) / sqrt_alphas_cumprod_t\n",
    "        \n",
    "        # DDIM update (eta=0 for deterministic sampling)\n",
    "        t_next = t - 1\n",
    "        t_next = torch.clamp(t_next, min=0)\n",
    "        sqrt_alphas_cumprod_next = torch.tensor(np.sqrt(alphas_cumprod), dtype=torch.float32, device=x.device)[t_next].unsqueeze(1)\n",
    "        sqrt_one_minus_alphas_cumprod_next = torch.tensor(np.sqrt(1 - alphas_cumprod), dtype=torch.float32, device=x.device)[t_next].unsqueeze(1)\n",
    "        x_next = sqrt_alphas_cumprod_next * x0_pred + sqrt_one_minus_alphas_cumprod_next * noise_pred\n",
    "        return x_next\n",
    "\n",
    "def sample_conditional(model, cond, num_steps=T):\n",
    "    model.eval()\n",
    "    batch_size = cond.size(0)\n",
    "    # Start from pure noise for the target\n",
    "    x = torch.randn((batch_size, 1), device=device)\n",
    "    for i in reversed(range(num_steps)):\n",
    "        t = torch.full((batch_size,), i, device=device, dtype=torch.long)\n",
    "        x = p_sample(model, x, cond, t)\n",
    "    return x\n",
    "\n",
    "# Example: sample for 5 random conditions from the dataset\n",
    "sample_conditions = torch.tensor(condition[:5], dtype=torch.float32).to(device)\n",
    "generated_time = sample_conditional(model, sample_conditions)\n",
    "print('Generated time_to_event:', generated_time.cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "We have now built a conditional diffusion model (following the TabDDPM idea of combining categorical and numerical features as conditions) that learns to generate the `time_to_event` for a patient given their full feature profile. This notebook illustrated the entire process from data preprocessing, model design, training, and sampling.\n",
    "\n",
    "Feel free to modify hyperparameters, experiment with the model architecture, or extend the diffusion process as needed for your application."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
